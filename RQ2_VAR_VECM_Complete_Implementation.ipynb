{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RQ2: SALES FORECASTING - VAR/VECM IMPLEMENTATION\n",
        "## Complete Pipeline with All Assumption Tests\n",
        "\n",
        "---\n",
        "\n",
        "### üìã Checklist Tahapan VAR/VECM:\n",
        "\n",
        "1. ‚úÖ **Data Preparation** - Aggregate revenue per product, add sentiment from reviews\n",
        "2. ‚úÖ **Uji Stasioneritas (ADF/KPSS)** - Check I(0) or I(1)\n",
        "3. ‚úÖ **Uji Lag Optimum (AIC/BIC/HQ)** - Determine optimal lag\n",
        "4. ‚úÖ **Uji Kointegrasi (Johansen)** - Check cointegration for VECM\n",
        "5. ‚úÖ **Estimasi Model (VAR/VECM)** - Fit appropriate model\n",
        "6. ‚úÖ **Uji Stabilitas (Eigenvalue)** - Check model stability\n",
        "7. ‚úÖ **Uji Diagnostik Residual**:\n",
        "   - Autokorelasi (LM Test)\n",
        "   - Normalitas (Jarque-Bera)\n",
        "   - Heteroskedastisitas (ARCH Test)\n",
        "8. ‚úÖ **Granger Causality** - Test causal relationships\n",
        "9. ‚úÖ **IRF & FEVD** - Dynamic analysis\n",
        "10. ‚úÖ **Forecasting** - Future predictions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Data Loading\n",
        "\n",
        "Assuming you've already loaded the data in previous cells, we'll continue from there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import additional libraries needed for RQ2\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from statsmodels.stats.diagnostic import het_arch, acorr_ljungbox\n",
        "from statsmodels.tsa.stattools import kpss\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "print(\"‚úÖ Libraries loaded for RQ2 analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation for VAR/VECM\n",
        "\n",
        "Prepare time series data with:\n",
        "- Daily revenue aggregation per product\n",
        "- Sentiment scores from reviews\n",
        "- Rating averages\n",
        "- Marketing spend (if applicable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create time series for VAR/VECM analysis\n",
        "def prepare_var_data(sales, reviews, products, marketing=None):\n",
        "    \"\"\"\n",
        "    Prepare multivariate time series for VAR/VECM\n",
        "    Includes revenue, sentiment, and other features\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"DATA PREPARATION FOR VAR/VECM\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # 1. Aggregate daily revenue per product\n",
        "    daily_revenue = sales.groupby(['date', 'product_id'])['revenue'].sum().reset_index()\n",
        "    revenue_ts = daily_revenue.pivot(index='date', columns='product_id', values='revenue')\n",
        "    revenue_ts = revenue_ts.fillna(0)\n",
        "    revenue_ts.columns = [f'P{col}' for col in revenue_ts.columns]\n",
        "    \n",
        "    print(f\"‚úì Revenue time series created: {revenue_ts.shape}\")\n",
        "    \n",
        "    # 2. Add sentiment analysis from reviews\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    \n",
        "    reviews_copy = reviews.copy()\n",
        "    reviews_copy['sentiment'] = reviews_copy['review'].apply(\n",
        "        lambda x: analyzer.polarity_scores(str(x))['compound'] if pd.notna(x) else 0\n",
        "    )\n",
        "    \n",
        "    # Aggregate daily sentiment\n",
        "    daily_sentiment = reviews_copy.groupby(['date', 'product_id']).agg({\n",
        "        'sentiment': 'mean',\n",
        "        'rating': 'mean',\n",
        "        'review_id': 'count'  # Review volume\n",
        "    }).reset_index()\n",
        "    daily_sentiment.rename(columns={'review_id': 'review_count'}, inplace=True)\n",
        "    \n",
        "    # Create sentiment time series\n",
        "    sentiment_ts = daily_sentiment.pivot(index='date', columns='product_id', values='sentiment')\n",
        "    sentiment_ts = sentiment_ts.fillna(0)\n",
        "    sentiment_ts.columns = [f'sent_P{col}' for col in sentiment_ts.columns]\n",
        "    \n",
        "    # Create rating time series\n",
        "    rating_ts = daily_sentiment.pivot(index='date', columns='product_id', values='rating')\n",
        "    rating_ts = rating_ts.fillna(method='ffill').fillna(3.0)  # Fill with neutral rating\n",
        "    rating_ts.columns = [f'rating_P{col}' for col in rating_ts.columns]\n",
        "    \n",
        "    print(f\"‚úì Sentiment features added: {sentiment_ts.shape[1]} variables\")\n",
        "    print(f\"‚úì Rating features added: {rating_ts.shape[1]} variables\")\n",
        "    \n",
        "    # 3. Combine all features\n",
        "    combined_ts = revenue_ts.copy()\n",
        "    \n",
        "    # Option to include sentiment and rating as exogenous variables\n",
        "    # For now, we'll focus on revenue only for main VAR/VECM\n",
        "    \n",
        "    print(f\"\\nüìä Final time series shape: {combined_ts.shape}\")\n",
        "    print(f\"Variables: {list(combined_ts.columns)}\")\n",
        "    \n",
        "    return combined_ts, sentiment_ts, rating_ts\n",
        "\n",
        "# Prepare data\n",
        "revenue_ts, sentiment_ts, rating_ts = prepare_var_data(sales, reviews, products, marketing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Analysis of Time Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize all 15 product revenue trends\n",
        "fig = make_subplots(\n",
        "    rows=5, cols=3,\n",
        "    subplot_titles=[f'Product {i}' for i in range(1, 16)],\n",
        "    vertical_spacing=0.06,\n",
        "    horizontal_spacing=0.08\n",
        ")\n",
        "\n",
        "for i, col in enumerate(revenue_ts.columns[:15], 1):\n",
        "    row = (i-1) // 3 + 1\n",
        "    col_idx = (i-1) % 3 + 1\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=revenue_ts.index,\n",
        "            y=revenue_ts[col],\n",
        "            mode='lines',\n",
        "            name=col,\n",
        "            line=dict(width=1),\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=row, col=col_idx\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    height=1000,\n",
        "    title_text=\"Daily Revenue Trends - 15 Products\",\n",
        "    showlegend=False\n",
        ")\n",
        "fig.update_xaxes(tickformat='%Y-%m')\n",
        "fig.update_yaxes(tickformat='$,.0f')\n",
        "fig.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nüìä Revenue Statistics Summary:\")\n",
        "summary = revenue_ts.describe().T[['mean', 'std', 'min', 'max']]\n",
        "summary['CV'] = summary['std'] / summary['mean']\n",
        "print(summary.sort_values('mean', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. TAHAP 1: Uji Stasioneritas (ADF & KPSS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stationarity_test_complete(df):\n",
        "    \"\"\"\n",
        "    Complete stationarity testing with ADF and KPSS\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"TAHAP 1: UJI STASIONERITAS (ADF & KPSS)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for col in df.columns[:15]:  # Test all 15 products\n",
        "        # ADF Test\n",
        "        adf_result = adfuller(df[col].dropna(), autolag='AIC')\n",
        "        \n",
        "        # KPSS Test\n",
        "        kpss_result = kpss(df[col].dropna(), regression='c', nlags='auto')\n",
        "        \n",
        "        # Determine order of integration\n",
        "        if adf_result[1] < 0.05 and kpss_result[1] > 0.05:\n",
        "            order = 'I(0)'\n",
        "            status = '‚úÖ Stationary'\n",
        "        elif adf_result[1] > 0.05 and kpss_result[1] < 0.05:\n",
        "            order = 'I(1)'\n",
        "            status = '‚ùå Non-stationary'\n",
        "        else:\n",
        "            order = 'Mixed'\n",
        "            status = '‚ö†Ô∏è Mixed results'\n",
        "        \n",
        "        results.append({\n",
        "            'Product': col,\n",
        "            'ADF_stat': adf_result[0],\n",
        "            'ADF_pval': adf_result[1],\n",
        "            'KPSS_stat': kpss_result[0],\n",
        "            'KPSS_pval': kpss_result[1],\n",
        "            'Order': order,\n",
        "            'Status': status\n",
        "        })\n",
        "    \n",
        "    # Create DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\nStationarity Test Results:\")\n",
        "    print(\"-\"*80)\n",
        "    print(results_df[['Product', 'ADF_pval', 'KPSS_pval', 'Order', 'Status']].to_string(index=False))\n",
        "    \n",
        "    # Summary\n",
        "    n_stationary = (results_df['Order'] == 'I(0)').sum()\n",
        "    n_nonstationary = (results_df['Order'] == 'I(1)').sum()\n",
        "    n_mixed = (results_df['Order'] == 'Mixed').sum()\n",
        "    \n",
        "    print(f\"\\nüìä Summary:\")\n",
        "    print(f\"  I(0) Stationary: {n_stationary}/15\")\n",
        "    print(f\"  I(1) Non-stationary: {n_nonstationary}/15\")\n",
        "    print(f\"  Mixed results: {n_mixed}/15\")\n",
        "    \n",
        "    # Decision\n",
        "    if n_nonstationary > 7:\n",
        "        print(f\"\\n‚ö†Ô∏è Majority non-stationary ‚Üí Test for cointegration (potential VECM)\")\n",
        "        use_vecm = True\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ Majority stationary ‚Üí Use VAR model\")\n",
        "        use_vecm = False\n",
        "    \n",
        "    return results_df, use_vecm\n",
        "\n",
        "# Test stationarity\n",
        "stationarity_results, potential_vecm = stationarity_test_complete(revenue_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. TAHAP 2: Uji Lag Optimum (AIC/BIC/HQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data for training and testing\n",
        "test_size = 30\n",
        "train_data = revenue_ts[:-test_size]\n",
        "test_data = revenue_ts[-test_size:]\n",
        "\n",
        "print(f\"Data Split:\")\n",
        "print(f\"  Training: {len(train_data)} days\")\n",
        "print(f\"  Testing: {len(test_data)} days\")\n",
        "\n",
        "def select_optimal_lag(df, maxlags=15):\n",
        "    \"\"\"\n",
        "    Select optimal lag using information criteria\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TAHAP 2: UJI LAG OPTIMUM (AIC/BIC/HQ)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    model = VAR(df)\n",
        "    lag_selection = model.select_order(maxlags=maxlags)\n",
        "    \n",
        "    print(\"\\nLag Selection Table:\")\n",
        "    print(lag_selection.summary())\n",
        "    \n",
        "    # Get optimal lags\n",
        "    optimal_lags = {\n",
        "        'AIC': lag_selection.aic,\n",
        "        'BIC': lag_selection.bic,\n",
        "        'FPE': lag_selection.fpe,\n",
        "        'HQIC': lag_selection.hqic\n",
        "    }\n",
        "    \n",
        "    print(\"\\nüìä Optimal Lag by Criterion:\")\n",
        "    for criterion, lag in optimal_lags.items():\n",
        "        print(f\"  {criterion}: {lag}\")\n",
        "    \n",
        "    # Use AIC\n",
        "    optimal_lag = optimal_lags['AIC']\n",
        "    print(f\"\\n‚úÖ Selected optimal lag: {optimal_lag} (based on AIC)\")\n",
        "    \n",
        "    return optimal_lag, lag_selection\n",
        "\n",
        "optimal_lag, lag_info = select_optimal_lag(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. TAHAP 3: Uji Kointegrasi (Johansen Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def johansen_cointegration_test(df, lag):\n",
        "    \"\"\"\n",
        "    Johansen cointegration test\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"TAHAP 3: UJI KOINTEGRASI (JOHANSEN TEST)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Perform test\n",
        "    result = coint_johansen(df, det_order=0, k_ar_diff=lag)\n",
        "    \n",
        "    # Extract statistics\n",
        "    trace_stat = result.lr1\n",
        "    cv_trace = result.cvt  # Critical values\n",
        "    max_eigen = result.lr2\n",
        "    cv_eigen = result.cvm\n",
        "    \n",
        "    print(\"\\nTrace Test Results:\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"{'H0: r<=':<10} {'Trace':<15} {'90% CV':<15} {'95% CV':<15} {'99% CV':<15} {'Result'}\")\n",
        "    print(\"-\"*80)\n",
        "    \n",
        "    n_coint = 0\n",
        "    for i in range(len(trace_stat)):\n",
        "        if trace_stat[i] > cv_trace[i, 1]:  # 95% level\n",
        "            result_str = \"Reject H0 ‚úì\"\n",
        "            n_coint = i + 1\n",
        "        else:\n",
        "            result_str = \"Fail to reject\"\n",
        "        \n",
        "        print(f\"{i:<10} {trace_stat[i]:<15.2f} {cv_trace[i,0]:<15.2f} \"\n",
        "              f\"{cv_trace[i,1]:<15.2f} {cv_trace[i,2]:<15.2f} {result_str}\")\n",
        "    \n",
        "    print(f\"\\nüìä Cointegration Results:\")\n",
        "    print(f\"  Number of cointegrating vectors: {n_coint}\")\n",
        "    \n",
        "    if n_coint > 0:\n",
        "        print(f\"  ‚úÖ Cointegration detected ‚Üí Use VECM with rank={n_coint}\")\n",
        "        use_vecm = True\n",
        "    else:\n",
        "        print(f\"  ‚ùå No cointegration ‚Üí Use VAR model\")\n",
        "        use_vecm = False\n",
        "    \n",
        "    return n_coint, use_vecm, result\n",
        "\n",
        "if potential_vecm:\n",
        "    coint_rank, use_vecm, johansen_result = johansen_cointegration_test(train_data, optimal_lag)\n",
        "else:\n",
        "    print(\"\\n‚úÖ Skipping cointegration test (data is stationary)\")\n",
        "    use_vecm = False\n",
        "    coint_rank = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. TAHAP 4: Estimasi Model (VAR atau VECM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TAHAP 4: ESTIMASI MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if use_vecm and coint_rank > 0:\n",
        "    print(f\"\\nüìà Estimating VECM Model\")\n",
        "    print(f\"  Cointegration rank: {coint_rank}\")\n",
        "    print(f\"  Lag order: {optimal_lag-1}\")\n",
        "    \n",
        "    # Fit VECM\n",
        "    vecm_model = VECM(train_data, k_ar_diff=optimal_lag-1, coint_rank=coint_rank, deterministic='ci')\n",
        "    vecm_fit = vecm_model.fit()\n",
        "    \n",
        "    # Get forecast\n",
        "    forecast = vecm_fit.predict(steps=len(test_data))\n",
        "    forecast_df = pd.DataFrame(forecast, index=test_data.index, columns=train_data.columns)\n",
        "    \n",
        "    model = vecm_fit\n",
        "    model_type = 'VECM'\n",
        "    \n",
        "    print(\"\\nVECM Model Summary:\")\n",
        "    print(f\"  Number of equations: {vecm_fit.neqs}\")\n",
        "    print(f\"  Sample size: {vecm_fit.nobs}\")\n",
        "    \n",
        "    # Show adjustment coefficients\n",
        "    print(\"\\nAdjustment Coefficients (Speed of Adjustment):\")\n",
        "    alpha = vecm_fit.alpha\n",
        "    for i in range(min(5, alpha.shape[0])):\n",
        "        print(f\"  Product {i+1}: {alpha[i,0]:.4f}\")\n",
        "    \n",
        "else:\n",
        "    print(f\"\\nüìà Estimating VAR Model\")\n",
        "    print(f\"  Lag order: {optimal_lag}\")\n",
        "    \n",
        "    # Check if differencing needed\n",
        "    if potential_vecm and not use_vecm:\n",
        "        print(\"  Note: Non-stationary but no cointegration ‚Üí Using differenced data\")\n",
        "        train_diff = train_data.diff().dropna()\n",
        "        test_diff = test_data.diff().dropna()\n",
        "        \n",
        "        var_model = VAR(train_diff)\n",
        "        var_fit = var_model.fit(optimal_lag)\n",
        "        \n",
        "        # Forecast differences\n",
        "        forecast_diff = var_fit.forecast(train_diff.values[-optimal_lag:], steps=len(test_diff))\n",
        "        \n",
        "        # Convert back to levels\n",
        "        forecast = np.zeros((len(test_data), len(train_data.columns)))\n",
        "        last_value = train_data.iloc[-1].values\n",
        "        forecast[0] = last_value + forecast_diff[0]\n",
        "        for i in range(1, len(forecast)):\n",
        "            forecast[i] = forecast[i-1] + forecast_diff[i]\n",
        "        \n",
        "        forecast_df = pd.DataFrame(forecast, index=test_data.index, columns=train_data.columns)\n",
        "        \n",
        "    else:\n",
        "        # Regular VAR\n",
        "        var_model = VAR(train_data)\n",
        "        var_fit = var_model.fit(optimal_lag)\n",
        "        \n",
        "        forecast = var_fit.forecast(train_data.values[-optimal_lag:], steps=len(test_data))\n",
        "        forecast_df = pd.DataFrame(forecast, index=test_data.index, columns=train_data.columns)\n",
        "    \n",
        "    model = var_fit\n",
        "    model_type = 'VAR'\n",
        "    \n",
        "    print(\"\\nVAR Model Summary:\")\n",
        "    print(f\"  Order: VAR({var_fit.k_ar})\")\n",
        "    print(f\"  Number of equations: {var_fit.neqs}\")\n",
        "    print(f\"  Log-likelihood: {var_fit.llf:.2f}\")\n",
        "    print(f\"  AIC: {var_fit.aic:.2f}\")\n",
        "    print(f\"  BIC: {var_fit.bic:.2f}\")\n",
        "\n",
        "# Calculate performance\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
        "\n",
        "mape = mean_absolute_percentage_error(test_data.values.flatten(), forecast_df.values.flatten())\n",
        "rmse = np.sqrt(mean_squared_error(test_data.values.flatten(), forecast_df.values.flatten()))\n",
        "\n",
        "print(f\"\\nüìä Forecast Performance:\")\n",
        "print(f\"  MAPE: {mape:.2%}\")\n",
        "print(f\"  RMSE: ${rmse:,.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. TAHAP 5: Uji Stabilitas Model (Eigenvalue/AR Root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TAHAP 5: UJI STABILITAS MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if model_type == 'VAR':\n",
        "    # Get roots\n",
        "    roots = model.roots\n",
        "    \n",
        "    print(f\"\\nStability Analysis:\")\n",
        "    print(f\"  Total roots: {len(roots)}\")\n",
        "    \n",
        "    # Check stability\n",
        "    unstable = []\n",
        "    for i, root in enumerate(roots):\n",
        "        if abs(root) >= 1:\n",
        "            unstable.append((i, root, abs(root)))\n",
        "    \n",
        "    # Show first 10 roots\n",
        "    print(\"\\nCharacteristic Roots (First 10):\")\n",
        "    print(f\"{'Index':<10} {'Root':<25} {'Modulus':<15} {'Status'}\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    for i in range(min(10, len(roots))):\n",
        "        modulus = abs(roots[i])\n",
        "        status = \"‚úÖ Stable\" if modulus < 1 else \"‚ùå UNSTABLE\"\n",
        "        print(f\"{i:<10} {str(roots[i]):<25} {modulus:<15.4f} {status}\")\n",
        "    \n",
        "    max_modulus = max(abs(roots))\n",
        "    is_stable = len(unstable) == 0\n",
        "    \n",
        "    print(f\"\\nüìä Stability Results:\")\n",
        "    print(f\"  Maximum modulus: {max_modulus:.4f}\")\n",
        "    print(f\"  Unstable roots: {len(unstable)}\")\n",
        "    print(f\"  Model stability: {'‚úÖ STABLE' if is_stable else '‚ùå UNSTABLE'}\")\n",
        "    \n",
        "    if not is_stable:\n",
        "        print(\"\\n‚ö†Ô∏è Model is unstable! Consider:\")\n",
        "        print(\"  - Reducing lag order\")\n",
        "        print(\"  - Checking for structural breaks\")\n",
        "        print(\"  - Data transformation\")\n",
        "    \n",
        "    # Plot roots\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    \n",
        "    # Unit circle\n",
        "    theta = np.linspace(0, 2*np.pi, 100)\n",
        "    ax.plot(np.cos(theta), np.sin(theta), 'k-', alpha=0.3, label='Unit Circle')\n",
        "    \n",
        "    # Plot roots\n",
        "    for root in roots:\n",
        "        color = 'blue' if abs(root) < 1 else 'red'\n",
        "        ax.scatter(root.real, root.imag, c=color, s=30, alpha=0.6)\n",
        "    \n",
        "    ax.set_xlim([-1.5, 1.5])\n",
        "    ax.set_ylim([-1.5, 1.5])\n",
        "    ax.set_xlabel('Real')\n",
        "    ax.set_ylabel('Imaginary')\n",
        "    ax.set_title(f'{model_type} Characteristic Roots')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
        "    ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"\\nVECM Stability:\")\n",
        "    print(\"  VECM models are stable by construction when cointegration exists\")\n",
        "    print(\"  ‚úÖ Model is stable\")\n",
        "    is_stable = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. TAHAP 6: Uji Diagnostik Residual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TAHAP 6: UJI DIAGNOSTIK RESIDUAL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get residuals\n",
        "residuals = pd.DataFrame(model.resid, columns=train_data.columns)\n",
        "\n",
        "# A. AUTOKORELASI (Ljung-Box Test)\n",
        "print(\"\\nA. AUTOKORELASI (Ljung-Box Test)\")\n",
        "print(\"-\"*60)\n",
        "print(\"Testing first 5 products:\")\n",
        "\n",
        "for i, col in enumerate(residuals.columns[:5]):\n",
        "    lb_test = acorr_ljungbox(residuals[col], lags=10, return_df=True)\n",
        "    min_pval = lb_test['lb_pvalue'].min()\n",
        "    status = \"‚úÖ No autocorrelation\" if min_pval > 0.05 else \"‚ö†Ô∏è Autocorrelation\"\n",
        "    print(f\"  {col}: Min p-value = {min_pval:.4f} | {status}\")\n",
        "\n",
        "# B. NORMALITAS (Jarque-Bera)\n",
        "print(\"\\nB. NORMALITAS (Jarque-Bera Test)\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "if model_type == 'VAR':\n",
        "    jb_test = model.test_normality()\n",
        "    print(f\"Multivariate test:\")\n",
        "    print(f\"  JB statistic: {jb_test.statistic:.4f}\")\n",
        "    print(f\"  P-value: {jb_test.pvalue:.4f}\")\n",
        "    print(f\"  Result: {'‚úÖ Normal' if jb_test.pvalue > 0.05 else '‚ö†Ô∏è Not normal'}\")\n",
        "\n",
        "print(\"\\nIndividual tests (first 5 products):\")\n",
        "for col in residuals.columns[:5]:\n",
        "    jb_stat, jb_pval = stats.jarque_bera(residuals[col])\n",
        "    status = \"‚úÖ Normal\" if jb_pval > 0.05 else \"‚ö†Ô∏è Not normal\"\n",
        "    print(f\"  {col}: JB = {jb_stat:.2f}, p-value = {jb_pval:.4f} | {status}\")\n",
        "\n",
        "# C. HETEROSKEDASTISITAS (ARCH Test)\n",
        "print(\"\\nC. HETEROSKEDASTISITAS (ARCH Test)\")\n",
        "print(\"-\"*60)\n",
        "print(\"Testing first 5 products:\")\n",
        "\n",
        "for col in residuals.columns[:5]:\n",
        "    arch_test = het_arch(residuals[col])\n",
        "    arch_stat = arch_test[0]\n",
        "    arch_pval = arch_test[1]\n",
        "    status = \"‚úÖ Homoskedastic\" if arch_pval > 0.05 else \"‚ö†Ô∏è Heteroskedastic\"\n",
        "    print(f\"  {col}: ARCH = {arch_stat:.2f}, p-value = {arch_pval:.4f} | {status}\")\n",
        "\n",
        "# D. DURBIN-WATSON TEST\n",
        "print(\"\\nD. DURBIN-WATSON TEST\")\n",
        "print(\"-\"*60)\n",
        "print(\"Testing first 5 products:\")\n",
        "\n",
        "for col in residuals.columns[:5]:\n",
        "    dw = durbin_watson(residuals[col])\n",
        "    if 1.5 < dw < 2.5:\n",
        "        status = \"‚úÖ No serial correlation\"\n",
        "    elif dw < 1.5:\n",
        "        status = \"‚ö†Ô∏è Positive serial correlation\"\n",
        "    else:\n",
        "        status = \"‚ö†Ô∏è Negative serial correlation\"\n",
        "    print(f\"  {col}: DW = {dw:.3f} | {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. TAHAP 7: Granger Causality Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TAHAP 7: UJI KAUSALITAS (GRANGER CAUSALITY)\")\n",
        "print(\"=\"*80)\n",
        "print(\"Testing for cannibalization: New products (P13-P15) ‚Üí Old products (P1-P5)\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "# Test new products against old products\n",
        "new_products = ['P13', 'P14', 'P15']\n",
        "old_products = ['P1', 'P2', 'P3', 'P4', 'P5']\n",
        "\n",
        "granger_results = []\n",
        "\n",
        "for new_prod in new_products:\n",
        "    for old_prod in old_products[:3]:  # Test against first 3\n",
        "        try:\n",
        "            # Prepare data\n",
        "            test_pair = train_data[[old_prod, new_prod]].dropna()\n",
        "            \n",
        "            # Granger test\n",
        "            gc_test = grangercausalitytests(test_pair, maxlag=5, verbose=False)\n",
        "            \n",
        "            # Get minimum p-value\n",
        "            p_values = [gc_test[lag][0]['ssr_ftest'][1] for lag in range(1, 6)]\n",
        "            min_pval = min(p_values)\n",
        "            best_lag = p_values.index(min_pval) + 1\n",
        "            \n",
        "            causality = \"YES ‚úì\" if min_pval < 0.05 else \"NO ‚úó\"\n",
        "            \n",
        "            granger_results.append({\n",
        "                'New': new_prod,\n",
        "                'Old': old_prod,\n",
        "                'Min_pval': min_pval,\n",
        "                'Lag': best_lag,\n",
        "                'Causes': causality\n",
        "            })\n",
        "            \n",
        "            print(f\"  {new_prod} ‚Üí {old_prod}: p={min_pval:.4f} (lag {best_lag}) | {causality}\")\n",
        "            \n",
        "        except:\n",
        "            print(f\"  {new_prod} ‚Üí {old_prod}: Test failed\")\n",
        "\n",
        "# Summary\n",
        "granger_df = pd.DataFrame(granger_results)\n",
        "n_significant = (granger_df['Causes'] == 'YES ‚úì').sum() if len(granger_df) > 0 else 0\n",
        "\n",
        "print(f\"\\nüìä Granger Causality Summary:\")\n",
        "print(f\"  Significant relationships: {n_significant}/{len(granger_df)}\")\n",
        "\n",
        "if n_significant > 0:\n",
        "    print(\"  ‚ö†Ô∏è Evidence of potential cannibalization!\")\n",
        "else:\n",
        "    print(\"  ‚úÖ No strong evidence of cannibalization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. TAHAP 8: Impulse Response Function (IRF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TAHAP 8: IMPULSE RESPONSE FUNCTION (IRF)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if model_type == 'VAR':\n",
        "    # Calculate IRF\n",
        "    irf = model.irf(10)\n",
        "    \n",
        "    print(\"\\nImpulse Response Analysis:\")\n",
        "    print(\"How shocks propagate across products\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    # Get IRF values\n",
        "    irf_values = irf.irfs\n",
        "    \n",
        "    # Analyze key relationships\n",
        "    print(\"\\nResponse to shock from Product 1:\")\n",
        "    for j in range(min(5, irf_values.shape[1])):\n",
        "        max_response = np.max(np.abs(irf_values[:, j, 0]))\n",
        "        peak_period = np.argmax(np.abs(irf_values[:, j, 0]))\n",
        "        print(f\"  Product {j+1}: Max response = {max_response:.4f} at period {peak_period}\")\n",
        "    \n",
        "    # Plot IRF for key products\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
        "    \n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            idx = i * 3 + j\n",
        "            if idx < 9:\n",
        "                # Response of product idx to shock from product 0\n",
        "                axes[i, j].plot(irf_values[:, idx, 0], 'b-', linewidth=2)\n",
        "                axes[i, j].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "                axes[i, j].set_title(f'Response of P{idx+1} to P1 shock')\n",
        "                axes[i, j].set_xlabel('Periods')\n",
        "                axes[i, j].set_ylabel('Response')\n",
        "                axes[i, j].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('Impulse Response Functions - Product Interactions', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Cumulative IRF\n",
        "    print(\"\\nCumulative Impulse Response (10 periods):\")\n",
        "    cum_irf = irf.cum_effects(10)\n",
        "    \n",
        "    for j in range(min(5, cum_irf.shape[1])):\n",
        "        cum_effect = cum_irf[9, j, 0]  # Period 10, from product 1\n",
        "        print(f\"  Product {j+1}: Cumulative effect = {cum_effect:.4f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"IRF analysis for VECM requires specialized implementation\")\n",
        "    print(\"Skipping for now...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. TAHAP 9: Forecast Error Variance Decomposition (FEVD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TAHAP 9: FORECAST ERROR VARIANCE DECOMPOSITION (FEVD)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if model_type == 'VAR':\n",
        "    # Calculate FEVD\n",
        "    fevd = model.fevd(10)\n",
        "    \n",
        "    print(\"\\nVariance Decomposition Analysis:\")\n",
        "    print(\"Which products explain variance in others\")\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    # Get decomposition\n",
        "    decomp = fevd.decomp\n",
        "    \n",
        "    # Analyze for first 5 products\n",
        "    for i in range(min(5, decomp.shape[0])):\n",
        "        print(f\"\\nProduct {i+1} variance explained by (at period 10):\")\n",
        "        \n",
        "        # Get contributions at period 10\n",
        "        contributions = decomp[i, 9, :]\n",
        "        \n",
        "        # Sort by contribution\n",
        "        sorted_idx = np.argsort(contributions)[::-1]\n",
        "        \n",
        "        # Show top 3 contributors\n",
        "        for j in sorted_idx[:3]:\n",
        "            print(f\"  Product {j+1}: {contributions[j]*100:.1f}%\")\n",
        "    \n",
        "    # Visualize FEVD\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    \n",
        "    for idx in range(4):\n",
        "        if idx < decomp.shape[0]:\n",
        "            row = idx // 2\n",
        "            col = idx % 2\n",
        "            \n",
        "            # Plot stacked area for top 5 contributors\n",
        "            data = decomp[idx, :, :5].T * 100  # Convert to percentage\n",
        "            \n",
        "            axes[row, col].stackplot(range(10), data,\n",
        "                                   labels=[f'P{i+1}' for i in range(5)],\n",
        "                                   alpha=0.7)\n",
        "            axes[row, col].set_xlabel('Periods')\n",
        "            axes[row, col].set_ylabel('Variance Share (%)')\n",
        "            axes[row, col].set_title(f'FEVD: Product {idx+1}')\n",
        "            axes[row, col].legend(loc='right', fontsize=8)\n",
        "            axes[row, col].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('Forecast Error Variance Decomposition', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"FEVD analysis for VECM requires specialized implementation\")\n",
        "    print(\"Skipping for now...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Future Forecast (30 days ahead)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FUTURE FORECAST (Next 30 Days)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Refit model on all data for future forecast\n",
        "all_data = pd.concat([train_data, test_data])\n",
        "\n",
        "if model_type == 'VAR':\n",
        "    # Refit VAR\n",
        "    final_model = VAR(all_data)\n",
        "    final_fit = final_model.fit(optimal_lag)\n",
        "    \n",
        "    # Forecast next 30 days\n",
        "    future_forecast = final_fit.forecast(all_data.values[-optimal_lag:], steps=30)\n",
        "    \n",
        "else:  # VECM\n",
        "    # Refit VECM\n",
        "    final_model = VECM(all_data, k_ar_diff=optimal_lag-1, coint_rank=coint_rank, deterministic='ci')\n",
        "    final_fit = final_model.fit()\n",
        "    \n",
        "    # Forecast next 30 days\n",
        "    future_forecast = final_fit.predict(steps=30)\n",
        "\n",
        "# Create future dates\n",
        "last_date = all_data.index[-1]\n",
        "future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=30)\n",
        "future_df = pd.DataFrame(future_forecast, index=future_dates, columns=all_data.columns)\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nüìä Future Forecast Summary (30 days):\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "total_revenue = future_df.sum().sum()\n",
        "daily_avg = future_df.sum(axis=1).mean()\n",
        "top_product = future_df.sum().idxmax()\n",
        "top_revenue = future_df.sum().max()\n",
        "\n",
        "print(f\"Total revenue (all products): ${total_revenue:,.0f}\")\n",
        "print(f\"Average daily revenue: ${daily_avg:,.0f}\")\n",
        "print(f\"Top product: {top_product} (${top_revenue:,.0f})\")\n",
        "\n",
        "# Growth rates\n",
        "last_30_days = all_data.iloc[-30:].sum()\n",
        "growth_rates = ((future_df.sum() - last_30_days) / last_30_days * 100).sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nüìà Expected Growth Rates vs Last 30 Days:\")\n",
        "print(\"Top 5 Growing Products:\")\n",
        "for product, growth in growth_rates.head(5).items():\n",
        "    print(f\"  {product}: {growth:+.1f}%\")\n",
        "\n",
        "print(\"\\nBottom 5 Products:\")\n",
        "for product, growth in growth_rates.tail(5).items():\n",
        "    print(f\"  {product}: {growth:+.1f}%\")\n",
        "\n",
        "# Visualize forecast\n",
        "fig = go.Figure()\n",
        "\n",
        "# Historical data (last 60 days)\n",
        "historical = all_data.iloc[-60:]\n",
        "\n",
        "# Plot top 5 products\n",
        "top_5_products = future_df.sum().nlargest(5).index\n",
        "\n",
        "for product in top_5_products:\n",
        "    # Historical\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=historical.index,\n",
        "        y=historical[product],\n",
        "        mode='lines',\n",
        "        name=f'{product} (Historical)',\n",
        "        line=dict(width=2)\n",
        "    ))\n",
        "    \n",
        "    # Forecast\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=future_df.index,\n",
        "        y=future_df[product],\n",
        "        mode='lines',\n",
        "        name=f'{product} (Forecast)',\n",
        "        line=dict(width=2, dash='dash')\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Revenue Forecast - Top 5 Products (Next 30 Days)',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Revenue ($)',\n",
        "    height=600,\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Final Summary and Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FINAL SUMMARY - RQ2 FORECASTING ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä MODEL SUMMARY:\")\n",
        "print(f\"  Model Type: {model_type}\")\n",
        "print(f\"  Optimal Lag: {optimal_lag}\")\n",
        "if model_type == 'VECM':\n",
        "    print(f\"  Cointegration Rank: {coint_rank}\")\n",
        "print(f\"  Model Stability: {'‚úÖ Stable' if is_stable else '‚ùå Unstable'}\")\n",
        "print(f\"  Test MAPE: {mape:.2%}\")\n",
        "print(f\"  Test RMSE: ${rmse:,.2f}\")\n",
        "\n",
        "print(f\"\\nüìà KEY INSIGHTS:\")\n",
        "\n",
        "# 1. Top performers\n",
        "top_3 = future_df.sum().nlargest(3)\n",
        "print(f\"\\n1. Top Revenue Generators (Next 30 days):\")\n",
        "for i, (product, revenue) in enumerate(top_3.items(), 1):\n",
        "    print(f\"   {i}. {product}: ${revenue:,.0f}\")\n",
        "\n",
        "# 2. Growth opportunities\n",
        "high_growth = growth_rates[growth_rates > 10]\n",
        "if len(high_growth) > 0:\n",
        "    print(f\"\\n2. High Growth Products (>10% growth):\")\n",
        "    for product, growth in high_growth.head(3).items():\n",
        "        print(f\"   ‚Ä¢ {product}: +{growth:.1f}%\")\n",
        "\n",
        "# 3. Cannibalization findings\n",
        "if n_significant > 0:\n",
        "    print(f\"\\n3. Cannibalization Alert:\")\n",
        "    print(f\"   ‚ö†Ô∏è {n_significant} significant causal relationships detected\")\n",
        "    print(f\"   New products may be cannibalizing old product sales\")\n",
        "else:\n",
        "    print(f\"\\n3. Cannibalization:\")\n",
        "    print(f\"   ‚úÖ No strong evidence of cannibalization\")\n",
        "\n",
        "# 4. Model diagnostics\n",
        "print(f\"\\n4. Model Diagnostics:\")\n",
        "if model_type == 'VAR' and hasattr(model, 'test_normality'):\n",
        "    if model.test_normality().pvalue > 0.05:\n",
        "        print(f\"   ‚úÖ Residuals are normally distributed\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è Residuals show non-normality\")\n",
        "\n",
        "print(f\"\\nüí° RECOMMENDATIONS:\")\n",
        "print(f\"\\n1. INVENTORY MANAGEMENT:\")\n",
        "print(f\"   ‚Ä¢ Increase stock for: {', '.join(top_3.index[:3])}\")\n",
        "print(f\"   ‚Ä¢ Monitor closely: Products with high growth potential\")\n",
        "\n",
        "print(f\"\\n2. MARKETING STRATEGY:\")\n",
        "if len(high_growth) > 0:\n",
        "    print(f\"   ‚Ä¢ Focus promotion on high-growth products\")\n",
        "    print(f\"   ‚Ä¢ Leverage momentum of {high_growth.index[0]}\")\n",
        "\n",
        "declining = growth_rates[growth_rates < -5]\n",
        "if len(declining) > 0:\n",
        "    print(f\"   ‚Ä¢ Review strategy for declining products: {', '.join(declining.index[:3])}\")\n",
        "\n",
        "print(f\"\\n3. PORTFOLIO OPTIMIZATION:\")\n",
        "if n_significant > 0:\n",
        "    print(f\"   ‚Ä¢ Address cannibalization between new and old products\")\n",
        "    print(f\"   ‚Ä¢ Consider product differentiation strategies\")\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ Product portfolio shows healthy independence\")\n",
        "    print(f\"   ‚Ä¢ Continue with current product mix\")\n",
        "\n",
        "print(f\"\\n4. FORECAST UPDATES:\")\n",
        "print(f\"   ‚Ä¢ Refit model weekly with new data\")\n",
        "print(f\"   ‚Ä¢ Monitor forecast accuracy (current MAPE: {mape:.2%})\")\n",
        "print(f\"   ‚Ä¢ Consider ensemble methods if MAPE > 15%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ RQ2 ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}